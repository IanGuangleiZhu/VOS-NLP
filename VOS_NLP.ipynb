{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VOS-NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3gE3K9uD5aO",
        "outputId": "6ab0b276-1589-4c7b-91e9-e9ce533c2653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.4.0-cp37-cp37m-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 33.0 MB/s \n",
            "\u001b[?25hCollecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 71.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.12.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.0->torchdata) (4.1.1)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 70.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Installing collected packages: urllib3, portalocker, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed portalocker-2.5.1 torchdata-0.4.0 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import mod\n",
        "import time\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext.datasets import AG_NEWS\n",
        "import numpy as np\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "from torch.utils.data import Sampler\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "train_iter = iter(AG_NEWS(split='train'))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'running on {device}')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "\n",
        "# build_vocab_from_iterator accepts iterator that yield list or iterator of tokens\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "# text pipeline converts a text string into a list of integers based on look up table\n",
        "# text -> tokenizer -> vocab indices\n",
        "# e.g. 'here is the an example' -> [475, 21, 2, 30, 5297]\n",
        "def text_pipeline(x): return vocab(tokenizer(x))\n",
        "# label pipline converts the string label into integers, e.g. '10' -> 9\n",
        "def label_pipeline(x): return int(x) - 1\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _text) in batch:\n",
        "        # if _label == 4:\n",
        "        #     pass\n",
        "        label_list.append(label_pipeline(_label))\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        processed_text = F.pad(\n",
        "            processed_text, (0, 50 - processed_text.shape[0]))\n",
        "        # if processed_text.shape[0] != 50:\n",
        "        # print(processed_text.shape[0] )\n",
        "        # print(len(tokenizer(_text)), processed_text.shape)\n",
        "        text_list.append(processed_text.unsqueeze(0))\n",
        "        # offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    # offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list, 0)\n",
        "    # text_list has shape (32, 50), i.e (batch_size, tokenized_text_size)\n",
        "    return label_list.to(device), text_list.to(device)\n",
        "\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class, theta, p=0.1):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.label_size = num_class\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.conv = nn.Conv1d(in_channels=embed_dim,\n",
        "                              out_channels=32, kernel_size=7, padding=\"same\")\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.fc = nn.Linear(32, num_class)\n",
        "        # for mlp, not used now\n",
        "        self.logistic_regression = nn.Sequential(\n",
        "            nn.Linear(1, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2, 1)\n",
        "        )\n",
        "        self.theta = theta\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        # add conv layer\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "        def init_energy(m):\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                m.bias.data.fill_(0.01)\n",
        "        self.logistic_regression.apply(init_energy)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # get embedding, output shape (batch_size, embed_size, text_length)\n",
        "        embedded = self.embedding(text)\n",
        "        embedded = embedded.transpose(1, 2)\n",
        "        # apply conv layer, output shape (batch_size, out_channels, text_length)\n",
        "        out = self.conv(embedded)\n",
        "        # max over text_length, output shape (batch_size, out_channels)\n",
        "        # this is the feature space that we want to sample from\n",
        "        out, _ = out.max(dim=-1)\n",
        "        feature = out.detach()\n",
        "        # apply dropout to avoid overfitting\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "        return out, feature\n",
        "\n",
        "    def energy(self, feature):\n",
        "        # to-do: do we need drop out?\n",
        "        # out = self.dropout(feature)\n",
        "        # print(f'feature has shape {feature.shape}')\n",
        "        logit = self.fc(feature)\n",
        "        return -1 * (torch.log(torch.tensor(1/3)) + torch.logsumexp(logit, dim=1, keepdim=True))\n",
        "\n",
        "    def mlp(self, x):\n",
        "        # return self.logistic_regression(x).squeeze()\n",
        "        return (self.theta * x).squeeze()\n",
        "        # out = self.input_fc(x.reshape((-1, 1)))\n",
        "        # out = self.hidden_fc(out)\n",
        "        # return self.output_fc(out).squeeze()\n",
        "\n",
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter])) - 1\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "theta = torch.tensor(1e-1, dtype=float, device=device, requires_grad=True)\n",
        "print(f'Vocab size is {vocab_size}')\n",
        "print(f'Embedding size is {emsize}')\n",
        "print(f'Num of class is {num_class}')\n",
        "print('-' * 59)\n",
        "model = TextClassificationModel(\n",
        "    vocab_size, emsize, num_class, theta).to(device)\n",
        "\n",
        "# consturct variables for GMM\n",
        "# shape (num_class, feature_dim=out_channels)\n",
        "class_mean = torch.from_numpy(np.zeros((num_class, 32))).float().to(device)\n",
        "class_cov = torch.eye(32).float()\n",
        "# assume num of classes is 4 - 1 = 3\n",
        "class_cov = class_cov.repeat(num_class, 1, 1).to(device)\n",
        "class_count = torch.from_numpy(np.zeros(num_class)).to(device)\n",
        "feature_batch = None\n",
        "\n",
        "\n",
        "def is_pos_def(x):\n",
        "    return np.all(np.linalg.eigvals(x) > 0)\n",
        "\n",
        "\n",
        "def is_pos_semi_def(x):\n",
        "    return np.all(np.linalg.eigvals(x) >= 0)\n",
        "\n",
        "\n",
        "def is_symmetric(x):\n",
        "    # return (x == x.transpose(0, 1)).all()\n",
        "    return (torch.abs(x - x.transpose(0, 1)) < 1e-5).all()\n",
        "\n",
        "\n",
        "def print_diff(x):\n",
        "    # return (x == x.transpose(0, 1)).all()\n",
        "    print((torch.abs(x - x.transpose(0, 1))).sum())\n",
        "\n",
        "\n",
        "# online update gradient with a batch (not training batch size) of feature observations\n",
        "def update_mean(class_mean, class_count, feature_batch, label_batch):\n",
        "    # class_mean of shape (num_class, feature_dim)\n",
        "    # feature_batch of shape (batch_size, feature_dim)\n",
        "    # label_batch of shape (batch_size)\n",
        "    for i in range(feature_batch.shape[0]):\n",
        "        label = label_batch[i]\n",
        "        class_mean[label] += (feature_batch[i] -\n",
        "                              class_mean[label]) / (1 + class_count[label])\n",
        "\n",
        "\n",
        "# online update covariance matrix\n",
        "def update_cov(class_mean, class_cov, class_count, feature_batch, label_batch):\n",
        "    # class_mean of shape (num_class, feature_dim)\n",
        "    # class_cov of shape (num_class, feature_dim, feature_dim)\n",
        "    # feature_batch of shape (batch_size, feature_dim)\n",
        "    # label_batch of shape (batch_size)\n",
        "    for i in range(feature_batch.shape[0]):\n",
        "        label = label_batch[i]\n",
        "        x = feature_batch[i]\n",
        "        u = class_mean[label]\n",
        "        t = class_count[label]\n",
        "        delta = (x - u).reshape((1, -1))\n",
        "        class_cov[label] *= t / (1 + t)\n",
        "        class_cov[label] += t / ((1 + t) ** 2) * delta.T @ delta\n",
        "\n",
        "\n",
        "def train(dataloader, class_count, class_mean, class_cov, feature_batch, beta, sample=True):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "    # loss = torch.tensor(0.0, dtype=float, device=device, requires_grad=True)\n",
        "\n",
        "    classidx_to_remove = 3\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        # seperate id and ood data\n",
        "        select_id = label != classidx_to_remove\n",
        "        label, text = label[select_id], text[select_id]\n",
        "\n",
        "        predicted_label, feature = model(text)\n",
        "        cls_loss = criterion(predicted_label, label)\n",
        "        # select features that has correct label prediction\n",
        "        feature_batch = feature[predicted_label.argmax(1) == label]\n",
        "        label_batch = label[predicted_label.argmax(1) == label]\n",
        "        class_count += torch.bincount(label_batch, minlength=num_class)\n",
        "        # only use 1000 samples for each class, here assume the number of classes is 3\n",
        "        exclude = torch.arange(0, num_class)[class_count > 1000]\n",
        "        for i in range(exclude.shape[0]):\n",
        "            select = [label_batch != exclude[i]]\n",
        "            label_batch = label_batch[select]\n",
        "            feature_batch = feature_batch[select]\n",
        "        # print(\n",
        "        #     f'epoch {epoch}, batch {idx}: pre-update symmetric {is_symmetric(class_cov[0].detach())}')\n",
        "        # code for calculate mean and covariance for GMM\n",
        "        update_mean(class_mean, class_count, feature_batch, label_batch)\n",
        "        update_cov(class_mean, class_cov, class_count,\n",
        "                   feature_batch, label_batch)\n",
        "\n",
        "        # only start sampling after some epoches\n",
        "        if sample:\n",
        "            m = MultivariateNormal(\n",
        "                class_mean[0], (class_cov[0] + class_cov[0].T)/2)\n",
        "            r = m.sample((1000,))\n",
        "            p = m.log_prob(r)\n",
        "            # the feature space outlier\n",
        "            _, indices = torch.max(p, 0)\n",
        "            outlier = r[indices]\n",
        "            # compute the uncertainty loss\n",
        "            cls_loss += -1 * beta * \\\n",
        "                F.logsigmoid(model.mlp(model.energy(outlier.unsqueeze(0))))\n",
        "            # to-do: update the formula, Done\n",
        "            # to-do: not sure how many iid features are needed\n",
        "            # to-do: gradient update?\n",
        "            cls_loss += -1 * beta * \\\n",
        "                F.logsigmoid(-1 * model.mlp(model.energy(feature[:1])))\n",
        "            # uncertainty_loss.backward()\n",
        "            # optimizer.step()\n",
        "            # L.register_hook(lambda grad: print(grad))\n",
        "            # if model.theta != 1.0:\n",
        "            #    print(model.theta)\n",
        "\n",
        "        cls_loss.backward()\n",
        "        # clip the gradient (really necessary?)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f} | theta {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                                              total_acc/total_count, model.theta))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def evaluate(dataloader, gamma):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    # measure the accuracy of detecting id data as id\n",
        "    total_id_acc, total_id_count = 0, 0\n",
        "    # measure the accuracy of detecting ood data as ood\n",
        "    total_ood_acc, total_ood_count = 0, 0\n",
        "\n",
        "    classidx_to_remove = 3\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            select = label != classidx_to_remove\n",
        "            label_id, text_id = label[select], text[select]\n",
        "            # to-do: fix this\n",
        "            label_ood, text_ood = label[~select], text[~select]\n",
        "\n",
        "            predicted_label, feature_id = model(text_id)\n",
        "            _, feature_ood = model(text_ood)\n",
        "\n",
        "            id = torch.sigmoid(model.mlp(model.energy(feature_id))) >= gamma\n",
        "            ood = torch.sigmoid(model.mlp(model.energy(feature_ood))) < gamma\n",
        "\n",
        "            # loss = criterion(predicted_label[id], label[id])\n",
        "            total_acc += (predicted_label[id].argmax(1)\n",
        "                          == label_id[id]).sum().item()\n",
        "            total_count += label_id[id].shape[0]\n",
        "            total_id_acc += id.sum()\n",
        "            total_id_count += label_id.shape[0]\n",
        "            total_ood_acc += ood.sum()\n",
        "            total_ood_count += label_ood.shape[0]\n",
        "\n",
        "    print(f'!! {total_ood_acc} {total_ood_count}')\n",
        "    # FPR 95\n",
        "    # the acc for classify id data, should be .95\n",
        "    # the acc for classify ood data\n",
        "    return total_acc/total_count, total_id_acc/total_id_count, total_ood_acc/total_ood_count\n",
        "\n",
        "\n",
        "def get_gamma(dataloader):\n",
        "    model.eval()\n",
        "    # the number of id data is about 4500\n",
        "    # will have trailing zeros in the end\n",
        "    l, c = 120000, 0\n",
        "    energies = torch.zeros(l)\n",
        "    tmp = torch.zeros(l)\n",
        "\n",
        "    classidx_to_remove = 3\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            select = label != classidx_to_remove\n",
        "            label, text = label[select], text[select]\n",
        "\n",
        "            _, feature = model(text)\n",
        "            # print(torch.sigmoid(model.mlp(model.energy(feature))).shape)\n",
        "            # print(label.shape[0], count + label.shape[0])\n",
        "            tmp[c: c + label.shape[0]] = model.mlp(model.energy(feature))\n",
        "            energies[c: c + label.shape[0]\n",
        "                     ] = torch.sigmoid(model.mlp(model.energy(feature)))\n",
        "            c += label.shape[0]\n",
        "    print(f'c = {c}')\n",
        "    energies = energies[:c]\n",
        "    tmp = tmp[:c]\n",
        "    # energies, _ = torch.sort(energies)\n",
        "    print(energies)\n",
        "    print(f'energy mean is {energies.mean().item()}')\n",
        "    print(tmp)\n",
        "    print(f'pre energy mean is {tmp.mean().item()}')\n",
        "    # should be 0.05 instead of 0.95!\n",
        "    gamma = torch.quantile(energies, 0.05)\n",
        "    return gamma\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 10  # epoch\n",
        "# the second term is for the theta/mlp, the first term is for the rest\n",
        "LR = [5, 1e-7]  # learning rate\n",
        "BATCH_SIZE = 32  # batch size for training\n",
        "beta = 0.1  # weight of uncertainty loss\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=LR[0])\n",
        "optimizer = torch.optim.SGD([{'params': model.parameters(), 'lr': LR[0]},\n",
        "                             {'params': model.theta, 'lr': LR[1]}])\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "sample = True\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader, class_count, class_mean,\n",
        "          class_cov, feature_batch, beta, sample=sample)\n",
        "    gamma = get_gamma(train_dataloader)\n",
        "    print(f'computed gamma is {gamma}')\n",
        "    accu_val, accu_id, accu_ood = evaluate(valid_dataloader, gamma)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "        scheduler.step()\n",
        "    else:\n",
        "        total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:6.3f} | id accuracy {:6.3f} | ood accuracy {:6.3f}'.format(epoch,\n",
        "                                                                                       time.time() - epoch_start_time,\n",
        "                                                                                       accu_val, accu_id, accu_ood))\n",
        "    print('-' * 59)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFbsAyvxD7n_",
        "outputId": "4e53bbd6-f8aa-4a5e-a042-06988a3f143d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on cuda\n",
            "Vocab size is 95811\n",
            "Embedding size is 64\n",
            "Num of class is 3\n",
            "-----------------------------------------------------------\n",
            "| epoch   1 |   500/ 3563 batches | accuracy    0.673 | theta    0.100\n",
            "| epoch   1 |  1000/ 3563 batches | accuracy    0.823 | theta    0.101\n",
            "| epoch   1 |  1500/ 3563 batches | accuracy    0.865 | theta    0.105\n",
            "| epoch   1 |  2000/ 3563 batches | accuracy    0.882 | theta    0.112\n",
            "| epoch   1 |  2500/ 3563 batches | accuracy    0.896 | theta    0.124\n",
            "| epoch   1 |  3000/ 3563 batches | accuracy    0.902 | theta    0.141\n",
            "| epoch   1 |  3500/ 3563 batches | accuracy    0.912 | theta    0.164\n",
            "c = 85488\n",
            "tensor([0.7378, 0.5532, 0.6467,  ..., 0.1215, 0.5783, 0.5037])\n",
            "energy mean is 0.39020803570747375\n",
            "tensor([ 1.0344,  0.2137,  0.6045,  ..., -1.9787,  0.3158,  0.0148])\n",
            "pre energy mean is -0.4785979986190796\n",
            "computed gamma is 0.19513055682182312\n",
            "!! 92 1488\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 82.94s | valid accuracy  0.908 | id accuracy  0.953 | ood accuracy  0.062\n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 3563 batches | accuracy    0.928 | theta    0.197\n",
            "| epoch   2 |  1000/ 3563 batches | accuracy    0.932 | theta    0.232\n",
            "| epoch   2 |  1500/ 3563 batches | accuracy    0.930 | theta    0.272\n",
            "| epoch   2 |  2000/ 3563 batches | accuracy    0.927 | theta    0.316\n",
            "| epoch   2 |  2500/ 3563 batches | accuracy    0.926 | theta    0.362\n",
            "| epoch   2 |  3000/ 3563 batches | accuracy    0.928 | theta    0.411\n",
            "| epoch   2 |  3500/ 3563 batches | accuracy    0.928 | theta    0.462\n",
            "c = 85488\n",
            "tensor([0.0618, 0.0016, 0.0093,  ..., 0.0026, 0.0177, 0.4573])\n",
            "energy mean is 0.08747771382331848\n",
            "tensor([-2.7200, -6.4491, -4.6685,  ..., -5.9537, -4.0157, -0.1714])\n",
            "pre energy mean is -4.020832538604736\n",
            "computed gamma is 0.0003214906028006226\n",
            "!! 54 1488\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 69.51s | valid accuracy  0.914 | id accuracy  0.941 | ood accuracy  0.036\n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 3563 batches | accuracy    0.948 | theta    0.520\n",
            "| epoch   3 |  1000/ 3563 batches | accuracy    0.945 | theta    0.572\n",
            "| epoch   3 |  1500/ 3563 batches | accuracy    0.944 | theta    0.625\n",
            "| epoch   3 |  2000/ 3563 batches | accuracy    0.947 | theta    0.679\n",
            "| epoch   3 |  2500/ 3563 batches | accuracy    0.942 | theta    0.733\n",
            "| epoch   3 |  3000/ 3563 batches | accuracy    0.940 | theta    0.788\n",
            "| epoch   3 |  3500/ 3563 batches | accuracy    0.945 | theta    0.843\n",
            "c = 85488\n",
            "tensor([1.0188e-02, 1.1984e-03, 1.7189e-05,  ..., 2.9703e-07, 4.7072e-08,\n",
            "        5.5771e-08])\n",
            "energy mean is 0.07117358595132828\n",
            "tensor([ -4.5763,  -6.7256, -10.9712,  ..., -15.0294, -16.8716, -16.7020])\n",
            "pre energy mean is -6.7274603843688965\n",
            "computed gamma is 1.0036901585408486e-06\n",
            "!! 30 1488\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 70.08s | valid accuracy  0.922 | id accuracy  0.949 | ood accuracy  0.020\n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 3563 batches | accuracy    0.960 | theta    0.904\n",
            "| epoch   4 |  1000/ 3563 batches | accuracy    0.956 | theta    0.959\n",
            "| epoch   4 |  1500/ 3563 batches | accuracy    0.951 | theta    1.014\n",
            "| epoch   4 |  2000/ 3563 batches | accuracy    0.952 | theta    1.069\n",
            "| epoch   4 |  2500/ 3563 batches | accuracy    0.954 | theta    1.124\n",
            "| epoch   4 |  3000/ 3563 batches | accuracy    0.956 | theta    1.180\n",
            "| epoch   4 |  3500/ 3563 batches | accuracy    0.951 | theta    1.235\n",
            "c = 85488\n",
            "tensor([7.2255e-06, 1.6372e-10, 1.6428e-04,  ..., 3.4476e-06, 6.8162e-09,\n",
            "        4.2829e-03])\n",
            "energy mean is 0.03594978526234627\n",
            "tensor([-11.8379, -22.5329,  -8.7138,  ..., -12.5778, -18.8040,  -5.4488])\n",
            "pre energy mean is -11.899391174316406\n",
            "computed gamma is 2.1836374375361345e-10\n",
            "!! 27 1488\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 69.99s | valid accuracy  0.930 | id accuracy  0.949 | ood accuracy  0.018\n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 3563 batches | accuracy    0.964 | theta    1.297\n",
            "| epoch   5 |  1000/ 3563 batches | accuracy    0.962 | theta    1.352\n",
            "| epoch   5 |  1500/ 3563 batches | accuracy    0.961 | theta    1.407\n",
            "| epoch   5 |  2000/ 3563 batches | accuracy    0.958 | theta    1.462\n",
            "| epoch   5 |  2500/ 3563 batches | accuracy    0.962 | theta    1.517\n",
            "| epoch   5 |  3000/ 3563 batches | accuracy    0.961 | theta    1.573\n",
            "| epoch   5 |  3500/ 3563 batches | accuracy    0.955 | theta    1.628\n",
            "c = 85488\n",
            "tensor([1.7991e-08, 5.2076e-08, 1.4362e-09,  ..., 1.6889e-06, 7.0315e-11,\n",
            "        1.0628e-09])\n",
            "energy mean is 0.021556051447987556\n",
            "tensor([-17.8334, -16.7706, -20.3613,  ..., -13.2914, -23.3780, -20.6623])\n",
            "pre energy mean is -18.80272674560547\n",
            "computed gamma is 1.6427155808724928e-15\n",
            "!! 25 1488\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 70.82s | valid accuracy  0.930 | id accuracy  0.943 | ood accuracy  0.017\n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/ 3563 batches | accuracy    0.967 | theta    1.690\n",
            "| epoch   6 |  1000/ 3563 batches | accuracy    0.969 | theta    1.745\n",
            "| epoch   6 |  1500/ 3563 batches | accuracy    0.966 | theta    1.800\n",
            "| epoch   6 |  2000/ 3563 batches | accuracy    0.964 | theta    1.855\n",
            "| epoch   6 |  2500/ 3563 batches | accuracy    0.963 | theta    1.910\n",
            "| epoch   6 |  3000/ 3563 batches | accuracy    0.962 | theta    1.965\n",
            "| epoch   6 |  3500/ 3563 batches | accuracy    0.963 | theta    2.020\n",
            "c = 85488\n",
            "tensor([3.3974e-05, 3.7467e-18, 2.0984e-11,  ..., 1.7980e-06, 1.8117e-15,\n",
            "        9.3908e-19])\n",
            "energy mean is 0.012222356162965298\n",
            "tensor([-10.2899, -40.1257, -24.5873,  ..., -13.2289, -33.9445, -41.5094])\n",
            "pre energy mean is -25.007102966308594\n",
            "computed gamma is 7.919343820286578e-20\n",
            "!! 23 1488\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 71.02s | valid accuracy  0.926 | id accuracy  0.944 | ood accuracy  0.015\n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/ 3563 batches | accuracy    0.978 | theta    2.032\n",
            "| epoch   7 |  1000/ 3563 batches | accuracy    0.981 | theta    2.038\n",
            "| epoch   7 |  1500/ 3563 batches | accuracy    0.984 | theta    2.043\n",
            "| epoch   7 |  2000/ 3563 batches | accuracy    0.984 | theta    2.049\n",
            "| epoch   7 |  2500/ 3563 batches | accuracy    0.984 | theta    2.054\n",
            "| epoch   7 |  3000/ 3563 batches | accuracy    0.983 | theta    2.060\n",
            "| epoch   7 |  3500/ 3563 batches | accuracy    0.985 | theta    2.065\n",
            "c = 85488\n",
            "tensor([6.2792e-14, 2.4999e-11, 3.2671e-23,  ..., 1.7772e-20, 6.4290e-14,\n",
            "        4.5479e-07])\n",
            "energy mean is 0.00458857836201787\n",
            "tensor([-30.3989, -24.4122, -51.7755,  ..., -45.4767, -30.3754, -14.6034])\n",
            "pre energy mean is -33.4482307434082\n",
            "computed gamma is 2.9563087508697455e-25\n",
            "!! 5 1488\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 69.85s | valid accuracy  0.937 | id accuracy  0.947 | ood accuracy  0.003\n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/ 3563 batches | accuracy    0.990 | theta    2.072\n",
            "| epoch   8 |  1000/ 3563 batches | accuracy    0.989 | theta    2.077\n",
            "| epoch   8 |  1500/ 3563 batches | accuracy    0.990 | theta    2.083\n",
            "| epoch   8 |  2000/ 3563 batches | accuracy    0.990 | theta    2.088\n",
            "| epoch   8 |  2500/ 3563 batches | accuracy    0.991 | theta    2.094\n",
            "| epoch   8 |  3000/ 3563 batches | accuracy    0.990 | theta    2.099\n",
            "| epoch   8 |  3500/ 3563 batches | accuracy    0.990 | theta    2.105\n",
            "c = 85488\n",
            "tensor([3.6838e-21, 4.1639e-17, 3.4639e-08,  ..., 4.1343e-14, 1.7751e-18,\n",
            "        2.4393e-11])\n",
            "energy mean is 0.0022567554842680693\n",
            "tensor([-47.0503, -37.7175, -17.1783,  ..., -30.8169, -40.8727, -24.4367])\n",
            "pre energy mean is -40.871341705322266\n",
            "computed gamma is 1.9455107838538547e-29\n",
            "!! 12 1488\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 69.79s | valid accuracy  0.940 | id accuracy  0.944 | ood accuracy  0.008\n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/ 3563 batches | accuracy    0.994 | theta    2.111\n",
            "| epoch   9 |  1000/ 3563 batches | accuracy    0.993 | theta    2.116\n",
            "| epoch   9 |  1500/ 3563 batches | accuracy    0.993 | theta    2.122\n",
            "| epoch   9 |  2000/ 3563 batches | accuracy    0.993 | theta    2.127\n",
            "| epoch   9 |  2500/ 3563 batches | accuracy    0.994 | theta    2.133\n",
            "| epoch   9 |  3000/ 3563 batches | accuracy    0.992 | theta    2.138\n",
            "| epoch   9 |  3500/ 3563 batches | accuracy    0.993 | theta    2.144\n",
            "c = 85488\n",
            "tensor([2.7469e-10, 1.5512e-15, 3.4205e-30,  ..., 4.8442e-15, 1.8804e-21,\n",
            "        1.0254e-22])\n",
            "energy mean is 0.0014851420419290662\n",
            "tensor([-22.0154, -34.0998, -67.8478,  ..., -32.9610, -47.7228, -50.6318])\n",
            "pre energy mean is -47.104610443115234\n",
            "computed gamma is 1.1672871962016401e-33\n",
            "!! 15 1488\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 69.17s | valid accuracy  0.937 | id accuracy  0.942 | ood accuracy  0.010\n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/ 3563 batches | accuracy    0.995 | theta    2.145\n",
            "| epoch  10 |  1000/ 3563 batches | accuracy    0.996 | theta    2.146\n",
            "| epoch  10 |  1500/ 3563 batches | accuracy    0.996 | theta    2.146\n",
            "| epoch  10 |  2000/ 3563 batches | accuracy    0.996 | theta    2.147\n",
            "| epoch  10 |  2500/ 3563 batches | accuracy    0.996 | theta    2.147\n",
            "| epoch  10 |  3000/ 3563 batches | accuracy    0.996 | theta    2.148\n",
            "| epoch  10 |  3500/ 3563 batches | accuracy    0.996 | theta    2.148\n",
            "c = 85488\n",
            "tensor([1.2763e-30, 1.3291e-23, 9.5872e-31,  ..., 6.4111e-25, 1.1931e-18,\n",
            "        1.6588e-24])\n",
            "energy mean is 0.0012262066593393683\n",
            "tensor([-68.8336, -52.6750, -69.1197,  ..., -55.7066, -41.2700, -54.7560])\n",
            "pre energy mean is -48.2587776184082\n",
            "computed gamma is 2.099805762683007e-34\n",
            "!! 10 1488\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 68.62s | valid accuracy  0.939 | id accuracy  0.943 | ood accuracy  0.007\n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}